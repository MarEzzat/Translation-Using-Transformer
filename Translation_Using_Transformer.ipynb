{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import io\n",
        "import re\n",
        "import unicodedata\n",
        "import time\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "eoobGL7Tnq5R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/SamirMoustafa/nmt-with-attention-for-ar-to-en/master/ara_.txt -O ara_.txt\n",
        "text_file = pathlib.Path('ara_.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb0_yInSnq2o",
        "outputId": "ef1223fb-c113-45e1-af0f-dfad24d73098"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-05 14:42:53--  https://raw.githubusercontent.com/SamirMoustafa/nmt-with-attention-for-ar-to-en/master/ara_.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 763396 (746K) [text/plain]\n",
            "Saving to: â€˜ara_.txtâ€™\n",
            "\n",
            "ara_.txt            100%[===================>] 745.50K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-05-05 14:42:53 (102 MB/s) - â€˜ara_.txtâ€™ saved [763396/763396]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_normalize(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFKC', s) if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(sentence, lang, is_target=False):\n",
        "    sentence = unicode_normalize(sentence.strip())\n",
        "    if lang == 'en':\n",
        "        sentence = sentence.lower()\n",
        "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    if lang == 'en':\n",
        "        sentence = re.sub(r\"[^a-zA-Z?.!,Â¿\\d]+\", \" \", sentence)\n",
        "    elif lang == 'ar':\n",
        "        sentence = re.sub(r\"[^\\u0600-\\u06FF\\d?.!,Â¿]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    if is_target:\n",
        "        sentence = f'[START] {sentence} [END]'\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "-CMOQqLSnq0v"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = io.open(text_file, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "sentence_pairs = [line.split('\\t') for line in text]\n",
        "valid_pairs = [pair for pair in sentence_pairs if len(pair) == 2]\n",
        "if len(valid_pairs) != len(sentence_pairs):\n",
        "    print(f\"Warning: {len(sentence_pairs) - len(valid_pairs)} lines ignored due to incorrect format.\")\n",
        "en_raw = [pair[0].strip() for pair in valid_pairs]\n",
        "ar_raw = [pair[1].strip() for pair in valid_pairs]\n",
        "\n",
        "en_processed = [preprocess_sentence(s, lang='en', is_target=True) for s in en_raw]\n",
        "ar_processed = [preprocess_sentence(s, lang='ar', is_target=False) for s in ar_raw]\n",
        "\n",
        "ar_lengths = [len(s.split(' ')) for s in ar_processed]\n",
        "en_lengths = [len(s.split(' ')) for s in en_processed]\n",
        "max_len_ar = max(ar_lengths)\n",
        "max_len_en = max(en_lengths)\n",
        "MAX_SEQUENCE_LENGTH = max(max_len_ar, max_len_en)"
      ],
      "metadata": {
        "id": "8AWe0-ZgnqyH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text_processor = tf.keras.layers.TextVectorization(standardize=None, ragged=True)\n",
        "output_text_processor = tf.keras.layers.TextVectorization(standardize=None, ragged=True)\n",
        "input_text_processor.adapt(ar_processed)\n",
        "output_text_processor.adapt(en_processed)\n",
        "INPUT_VOCAB_SIZE = input_text_processor.vocabulary_size()\n",
        "TARGET_VOCAB_SIZE = output_text_processor.vocabulary_size()\n",
        "\n",
        "BUFFER_SIZE = len(ar_processed)\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "fHRBpXS6nqvG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(ar_batch, en_batch):\n",
        "    ar_seq = input_text_processor(ar_batch)\n",
        "    en_seq = output_text_processor(en_batch)\n",
        "    en_input = en_seq[:, :-1]\n",
        "    en_label = en_seq[:, 1:]\n",
        "    ar_seq_padded = ar_seq.to_tensor(default_value=0, shape=[None, MAX_SEQUENCE_LENGTH])\n",
        "    en_input_padded = en_input.to_tensor(default_value=0, shape=[None, MAX_SEQUENCE_LENGTH])\n",
        "    en_label_padded = en_label.to_tensor(default_value=0, shape=[None, MAX_SEQUENCE_LENGTH])\n",
        "    return (ar_seq_padded, en_input_padded), en_label_padded\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((ar_processed, en_processed))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).map(prepare_batch, tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_size = int(0.9 * len(ar_processed))\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)"
      ],
      "metadata": {
        "id": "vfzAs8INnqsj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(length, depth):\n",
        "    depth = depth / 2\n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
        "    angle_rates = 1 / (10000**depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "    pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1)\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "    def compute_mask(self, *args, **kwargs):\n",
        "        return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "    def call(self, x):\n",
        "        length = tf.shape(x)[1]\n",
        "        x = self.embedding(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "079kaHaWnqqL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "    if mask is not None:\n",
        "        mask = tf.cast(mask, dtype=scaled_attention_logits.dtype)\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "73qKkugOnqni"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self,*, d_model, num_heads, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.dropout_rate = dropout_rate\n",
        "        assert d_model % self.num_heads == 0\n",
        "        self.depth = d_model // self.num_heads\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
        "        output = self.dense(concat_attention)\n",
        "        output = self.dropout(output)\n",
        "        return output, attention_weights"
      ],
      "metadata": {
        "id": "5trzIGTWnqio"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PointWiseFeedForwardNetwork(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.seq = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(dff, activation='relu'),\n",
        "            tf.keras.layers.Dense(d_model),\n",
        "            tf.keras.layers.Dropout(dropout_rate)\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        return self.seq(x, training=training)\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        self.ffn = PointWiseFeedForwardNetwork(d_model=d_model, dff=dff, dropout_rate=dropout_rate)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        attn_output, _ = self.mha(x, x, x, mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return out2\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        self.ffn = PointWiseFeedForwardNetwork(d_model=d_model, dff=dff, dropout_rate=dropout_rate)\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, mask=look_ahead_mask)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, mask=padding_mask)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)\n",
        "        ffn_output = self.ffn(out2)\n",
        "        out3 = self.layernorm3(ffn_output + out2)\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "htZ4--3knqgo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OD5n11ydnpSI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "        self.enc_layers = [EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
        "                          for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.pos_embedding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "        return x\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, dropout_rate=dropout_rate)\n",
        "                          for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training=False, look_ahead_mask=None, padding_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "        x = self.pos_embedding(x)\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training=training,\n",
        "                                                  look_ahead_mask=look_ahead_mask,\n",
        "                                                  padding_mask=padding_mask)\n",
        "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "                 input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                              num_heads=num_heads, dff=dff,\n",
        "                              vocab_size=input_vocab_size, dropout_rate=dropout_rate)\n",
        "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                              num_heads=num_heads, dff=dff,\n",
        "                              vocab_size=target_vocab_size, dropout_rate=dropout_rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        inp, tar = inputs\n",
        "        enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n",
        "        enc_output = self.encoder(inp, training=training, mask=enc_padding_mask)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training=training,\n",
        "                                                    look_ahead_mask=look_ahead_mask,\n",
        "                                                    padding_mask=dec_padding_mask)\n",
        "        final_output = self.final_layer(dec_output)\n",
        "        return final_output, attention_weights\n",
        "\n",
        "transformer = Transformer(num_layers=4, d_model=128, num_heads=8,\n",
        "                         dff=512, input_vocab_size=INPUT_VOCAB_SIZE, target_vocab_size=TARGET_VOCAB_SIZE,\n",
        "                         dropout_rate=0.1)"
      ],
      "metadata": {
        "id": "hugN7iZ3qSzY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, dtype=tf.float32)\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "metadata": {
        "id": "YDmjG-kLqSxR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(d_model=128)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')"
      ],
      "metadata": {
        "id": "t0_hEQGtqSuq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inputs):\n",
        "    (inp, tar_inp), tar_real = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer([inp, tar_inp], training=True)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    train_loss(loss)\n",
        "\n",
        "@tf.function\n",
        "def val_step(inputs):\n",
        "    (inp, tar_inp), tar_real = inputs\n",
        "    predictions, _ = transformer([inp, tar_inp], training=False)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "    val_loss(loss)"
      ],
      "metadata": {
        "id": "htb8-jAeqSsE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'transformer_model.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "train_batches = len(list(train_dataset))\n",
        "val_batches = len(list(val_dataset))\n",
        "\n",
        "if transformer is None:\n",
        "    raise ValueError(\"Transformer model was not properly initialized\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss.reset_state()\n",
        "    val_loss.reset_state()\n",
        "\n",
        "    with tqdm(total=train_batches, desc=f'Epoch {epoch + 1}/{EPOCHS}', unit='batch') as pbar:\n",
        "        for (batch, inputs) in enumerate(train_dataset):\n",
        "            train_step(inputs)\n",
        "            pbar.set_postfix({'Batch Loss': float(train_loss.result())})\n",
        "            pbar.update(1)\n",
        "\n",
        "    for inputs in val_dataset:\n",
        "        val_step(inputs)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    print(f\"Epoch {epoch + 1} | Train Loss: {train_loss.result():.4f} | Time: {epoch_time:.2f} sec\")\n",
        "\n",
        "\n",
        "    current_learning_rate = learning_rate(optimizer.iterations)\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Learning Rate: {current_learning_rate:.6f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUXMAOAVrPjY",
        "outputId": "ecf3e253-5a1e-48ca-a680-8bd3d106dcd8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.81batch/s, Batch Loss=1.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 1.3657 | Time: 11.46 sec\n",
            "Epoch 1 Loss 1.3657 Learning Rate: 0.001233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=1.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 1.2487 | Time: 20.61 sec\n",
            "Epoch 2 Loss 1.2487 Learning Rate: 0.001291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 1.1474 | Time: 20.60 sec\n",
            "Epoch 3 Loss 1.1474 Learning Rate: 0.001350\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 1.0577 | Time: 20.60 sec\n",
            "Epoch 4 Loss 1.0577 Learning Rate: 0.001392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.90batch/s, Batch Loss=0.954]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 0.9539 | Time: 11.38 sec\n",
            "Epoch 5 Loss 0.9539 Learning Rate: 0.001364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.851]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Train Loss: 0.8514 | Time: 20.58 sec\n",
            "Epoch 6 Loss 0.8514 Learning Rate: 0.001337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.93batch/s, Batch Loss=0.752]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Train Loss: 0.7517 | Time: 11.42 sec\n",
            "Epoch 7 Loss 0.7517 Learning Rate: 0.001312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.19batch/s, Batch Loss=0.676]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Train Loss: 0.6760 | Time: 20.63 sec\n",
            "Epoch 8 Loss 0.6760 Learning Rate: 0.001289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.594]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Train Loss: 0.5939 | Time: 20.59 sec\n",
            "Epoch 9 Loss 0.5939 Learning Rate: 0.001266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.83batch/s, Batch Loss=0.536]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Train Loss: 0.5359 | Time: 11.44 sec\n",
            "Epoch 10 Loss 0.5359 Learning Rate: 0.001245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.77batch/s, Batch Loss=0.491]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Train Loss: 0.4908 | Time: 11.47 sec\n",
            "Epoch 11 Loss 0.4908 Learning Rate: 0.001225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.87batch/s, Batch Loss=0.441]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Train Loss: 0.4413 | Time: 11.40 sec\n",
            "Epoch 12 Loss 0.4413 Learning Rate: 0.001205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.97batch/s, Batch Loss=0.404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Train Loss: 0.4035 | Time: 11.33 sec\n",
            "Epoch 13 Loss 0.4035 Learning Rate: 0.001187\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.369]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Train Loss: 0.3692 | Time: 20.68 sec\n",
            "Epoch 14 Loss 0.3692 Learning Rate: 0.001170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.19batch/s, Batch Loss=0.343]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Train Loss: 0.3435 | Time: 20.60 sec\n",
            "Epoch 15 Loss 0.3435 Learning Rate: 0.001153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.95batch/s, Batch Loss=0.317]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Train Loss: 0.3170 | Time: 11.34 sec\n",
            "Epoch 16 Loss 0.3170 Learning Rate: 0.001137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.297]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Train Loss: 0.2965 | Time: 20.59 sec\n",
            "Epoch 17 Loss 0.2965 Learning Rate: 0.001121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.282]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Train Loss: 0.2817 | Time: 20.60 sec\n",
            "Epoch 18 Loss 0.2817 Learning Rate: 0.001106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.96batch/s, Batch Loss=0.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Train Loss: 0.2596 | Time: 11.33 sec\n",
            "Epoch 19 Loss 0.2596 Learning Rate: 0.001092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.19batch/s, Batch Loss=0.243]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Train Loss: 0.2431 | Time: 20.69 sec\n",
            "Epoch 20 Loss 0.2431 Learning Rate: 0.001078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.94batch/s, Batch Loss=0.23]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | Train Loss: 0.2298 | Time: 11.36 sec\n",
            "Epoch 21 Loss 0.2298 Learning Rate: 0.001065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.93batch/s, Batch Loss=0.219]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | Train Loss: 0.2194 | Time: 11.35 sec\n",
            "Epoch 22 Loss 0.2194 Learning Rate: 0.001052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.19batch/s, Batch Loss=0.208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | Train Loss: 0.2077 | Time: 20.60 sec\n",
            "Epoch 23 Loss 0.2077 Learning Rate: 0.001040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.19batch/s, Batch Loss=0.195]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | Train Loss: 0.1952 | Time: 20.59 sec\n",
            "Epoch 24 Loss 0.1952 Learning Rate: 0.001028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.188]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | Train Loss: 0.1876 | Time: 20.60 sec\n",
            "Epoch 25 Loss 0.1876 Learning Rate: 0.001017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.89batch/s, Batch Loss=0.176]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 | Train Loss: 0.1763 | Time: 11.47 sec\n",
            "Epoch 26 Loss 0.1763 Learning Rate: 0.001005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 14.76batch/s, Batch Loss=0.173]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 | Train Loss: 0.1725 | Time: 11.54 sec\n",
            "Epoch 27 Loss 0.1725 Learning Rate: 0.000995\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:11<00:00, 15.01batch/s, Batch Loss=0.159]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 | Train Loss: 0.1595 | Time: 11.37 sec\n",
            "Epoch 28 Loss 0.1595 Learning Rate: 0.000984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.19batch/s, Batch Loss=0.154]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 | Train Loss: 0.1535 | Time: 20.60 sec\n",
            "Epoch 29 Loss 0.1535 Learning Rate: 0.000974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 168/168 [00:20<00:00,  8.20batch/s, Batch Loss=0.153]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 | Train Loss: 0.1533 | Time: 20.59 sec\n",
            "Epoch 30 Loss 0.1533 Learning Rate: 0.000964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, transformer, input_text_processor, output_text_processor, max_length=MAX_SEQUENCE_LENGTH):\n",
        "    if transformer is None:\n",
        "        raise ValueError(\"Transformer model is None in evaluate function\")\n",
        "    sentence = preprocess_sentence(sentence, lang='ar')\n",
        "    input_tensor = input_text_processor([sentence])\n",
        "    input_tensor = input_tensor.to_tensor(shape=[1, max_length])\n",
        "    start_token = output_text_processor(['[START]']).numpy()[0][0]\n",
        "    end_token = output_text_processor(['[END]']).numpy()[0][0]\n",
        "    output_tokens = [start_token]\n",
        "    for i in range(max_length):\n",
        "        decoder_input = tf.expand_dims(output_tokens, 0)\n",
        "        predictions, _ = transformer([input_tensor, decoder_input], training=False)\n",
        "        next_token = tf.argmax(predictions[0, -1, :], axis=-1).numpy()\n",
        "        if next_token == end_token:\n",
        "            break\n",
        "        output_tokens.append(next_token)\n",
        "    predicted_tokens = tf.constant(output_tokens)\n",
        "    predicted_words = output_text_processor.get_vocabulary()\n",
        "    translated = ' '.join([predicted_words[token] for token in output_tokens[1:]])\n",
        "    return translated\n",
        "\n",
        "def translate(sentence, transformer):\n",
        "    translation = evaluate(sentence, transformer, input_text_processor, output_text_processor)\n",
        "    print(f'\\nðŸŸ¢ Arabic Input     : {sentence}')\n",
        "    print(f'ðŸ”µ English Translation : {translation}')"
      ],
      "metadata": {
        "id": "rCszAa9wrPhm"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\", transformer)\n",
        "translate(\"Ø£Ù†Ø§ Ø£Ø­Ø¨Ùƒ\", transformer)\n",
        "translate(\"Ù‡Ù„ ØªØ­Ø¨ ÙˆØ§Ù„Ø¯ÙƒØŸ\", transformer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgrVe6DArPe5",
        "outputId": "250df784-42c3-4505-cb08-0e6a73f3d2fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŸ¢ Arabic Input     : ÙƒÙŠÙ Ø­Ø§Ù„ÙƒØŸ\n",
            "ðŸ”µ English Translation : how are you doing ?\n",
            "\n",
            "ðŸŸ¢ Arabic Input     : Ø£Ù†Ø§ Ø£Ø­Ø¨Ùƒ\n",
            "ðŸ”µ English Translation : i love you .\n",
            "\n",
            "ðŸŸ¢ Arabic Input     : Ù‡Ù„ ØªØ­Ø¨ ÙˆØ§Ù„Ø¯ÙƒØŸ\n",
            "ðŸ”µ English Translation : do you like your father ?\n"
          ]
        }
      ]
    }
  ]
}